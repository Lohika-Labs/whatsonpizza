# What's On Pizza Project

## Task

Build an ML model that would look at a picture of a pizza and output a list of possible pizza name.

So far, the model doesn't perform well, most likely due to a low-quality dataset.

We trained a model for a single-label classification, where the output just the name of the pizza.

The dataset and the snapshots of the trained model are not in the git repository because they are too large. To access these, connect to our server with the password `User123`:

``` ssh otkach@172.22.46.67 ```

There's a folder `whatsonpizza` in the home directory.

## Dataset

Dataset is a collection of images of pizzas.

Other files in the dataset:

`pizza.json` - a JSON file connecting the names of the pizzas and lists of ingredients on them with the image names.

`train_data.lst`, `test_data.lst`, `val_data.lst` - files generated from the JSON file. They are used to iterate over the data during training. The first column in the file is the image id, the last one - path to the image (in our case - just its name), and the columns in between are a one-hot encoding vector. Each number in the vector corresponds to an ingredient among all possible ingredients that can be in the output, and the value is 0 if the ingredient is not on the pizza, 1 - if it is.

These files are generated using the `gen_files.py` script.

`test_data_single.lst`, `train_data_single.lst`, `val_data_single.lst` - the same format, except this was used to see if the same dataset would perform better on a single-label classification (the classes are just pizza names). So instead of one-hot encoded vectors, the classes are signified by a class id.

Same names but with the `.rec` extension - these are compressed images along with the information from the `.lst` files. These are generated by an MXnet rec tool, and can be iterated over during training using a different iterator.

`cats.txt` - 'cats' is for categories. This is a list of all possible ingredients, used to later connect the id in the output vector to the actual word.

## The code

The main files used during the training of the neural network are `train_multilabel.py` and `crossentropy.py`. The cross-entropy file contains a custom MXnet layer that handles the multilabel classification. The 'train_multilabel' file uploads a pretrained neural network model (resnet-50) and then tunes its parameters to our needs.

The pretrained model is in the folder `model`. Before training, its output layer is replaced with the custom layer in the `crossentropy.py` file. The tuned model is saved in the folder `snapshots` after each epoch of training.

In order to launch the training, we used the file `run_train.sh`, where you can specify all needed parameters, like paths to the dataset, number of epochs, a list of GPU threads (if this parameter is empty, the CPU will be used), etc.

## Evaluation

`evaluation.py` calculates the accuracy, recall, precision and F1 for each snapshot, then saves these metrics in the file `results.json`.

`demo.py` will take a random image from the dataset, predict its ingredients using the best snapshot, then display the image and the list of ingredients.
